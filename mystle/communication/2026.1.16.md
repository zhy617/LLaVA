师兄想跟你同步下我这边的测试现状，想请你帮我对齐一下之前你那套测试逻辑。

下面是 AI 帮我总结的问题，我修改了下内容和格式，比较符合我的问题了：

**我目前的测试流程是这样的：**

1）数据：DriveLM `v1_1_val_nus_q_only.json`，每个 scene 的 `key_frames` 按 frame_id 顺序跑，默认测试前 5 个 scene。

2）模型：`llava-v1.6-vicuna-7b`，一次性 load 模型，不在每个 frame 重新 load。

3）输入：每个 frame 读 6 个视角（front/front-left/front-right/back/back-left/back-right），并在 prompt 里显式写相机方位，先插入起始 image prompt:

`"The following images are provided from different camera angles:\n"`

然后再插入每张图片，比如：

* CAM_FRONT: `- CAM_FRONT: <image>`
* CAM_FRONT_LEFT: `- CAM_FRONT_LEFT: <image>`

然后再拼上师兄给的那一段 `initial_prompt`（描述场景 + 预测自车方向/速度）。

4）对话：我现在是**每个 frame 都重新初始化 conv**（`conv_templates[...] .copy()`），避免对话历史累积。
  
5）连续生成相关：我在代码里做了两种模式对比：
* **模式A（逐帧独立）**：`use_memory_idea=False`，并把 `model.is_first_frame` 设成每帧都当第一帧。
* **模式B（连续生成）**：`use_memory_idea=True`，只在第一帧 `model.is_first_frame=True`，后面为 False；调用的是 `model.generate_with_mem_ppl_continue_gen(...)`（use_cache=True）。

**我观察到的问题：**

1）当我按“连续生成”（模式B）跑时，会更容易出现**幻觉**和**输出格式不稳定**（比如不按我要求的 direction/speed 输出，或者描述跑偏）。

2）当我改成“每帧都当第一帧”（模式A，逐帧独立）时，幻觉明显减少，输出也更规范。

3）所以我现在怀疑三点：

* 可能是 **memory/continue 的状态**在 frame 间累积了不该累积的东西（即便我 conv 每帧重置了）

* 也可能是我现在的 **prompt 拼法**（带 camera 名称的 `<image>` 列表 + initial_prompt）和师兄之前的测试方式不一致，导致 continue 的行为异常。（但我之前也测试了师兄的 prompt 拼法，现象类似）

* 我修复了师兄代码中的当复用性为 0 时，current_sequences 为空，导致后续推理 inputs 取值错误的 bug。可能是我修复 bug 的方式不太对？

**我现在想请师兄帮忙的点：**

1）师兄之前的测试里，“连续生成”那套的**核心测试逻辑**是怎么写的？尤其是：

* 每帧之间你是怎么处理 `conv / input_ids / is_first_frame / cache` 的？

* prompt 是每帧都完整指令，还是后续帧只给 image token？

2）如果方便的话，能不能把你之前的关键代码片段（或伪代码）发我看一下，我想逐项对比差异，定位到底是我 prompt 还是状态管理的问题。

（我把我目前的测试代码也整理好了，随时可以贴给师兄看/发文件。）

